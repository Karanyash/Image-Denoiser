{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1-UNcW7_YwLrlnFzmTccFES4d1Qt-57yy","authorship_tag":"ABX9TyMhkZXy4gsp1itEcS6sbb5T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os\n","import sys\n","import argparse\n","import json\n","\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from tqdm import tqdm\n","import keras\n","from keras import backend as K\n","from keras import activations, initializers, regularizers, constraints, metrics\n","from keras.datasets import cifar10\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential, Model\n","from keras.layers import (Dense, Dropout, Activation, Flatten, Reshape, Layer,\n","                          BatchNormalization, LocallyConnected2D,\n","                          ZeroPadding2D, Conv2D, MaxPooling2D, Conv2DTranspose,\n","                          GaussianNoise, UpSampling2D, Input)\n","\n","\n"],"metadata":{"id":"dkWsTGQLKQf1","executionInfo":{"status":"ok","timestamp":1687765284778,"user_tz":-330,"elapsed":2951,"user":{"displayName":"6112_Yash_Raj","userId":"06390485470456786871"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["def load_images(path):\n","\n","    image_set = {}\n","    for root, dirs, files in os.walk(path):\n","        if root == path:\n","            categories = sorted(dirs)\n","            image_set = {cat: [] for cat in categories}\n","        else:\n","            image_set[os.path.basename(root)] = sorted(files)\n","\n","    n_cat_images = {cat: len(files) for (cat, files) in image_set.items()}\n","    n_images = sum(n_cat_images.values())\n","    image_dims = plt.imread(os.path.join(path, categories[0],\n","                            image_set[categories[0]][0])).shape\n","\n","    print(image_dims)\n","    # X = np.zeros((n_images, *image_dims), dtype='float32')\n","    X = np.zeros((n_images, image_dims[0], image_dims[1], 1), dtype='float32')\n","    y = np.zeros((n_images, len(categories)), dtype=int)\n","    # y = np.zeros(n_images, dtype=int)\n","\n","    tally = 0\n","    for c, (cat, files) in enumerate(tqdm(image_set.items(), desc=path)):\n","        for i, image in enumerate(files):\n","            cimg = plt.imread(os.path.join(path, cat, image))\n","            X[i+tally] = np.expand_dims(cv2.cvtColor(cimg, cv2.COLOR_BGR2GRAY), axis=-1)\n","        y[tally:tally+len(files), c] = True\n","        tally += len(files)\n","\n","    shuffle = np.random.permutation(y.shape[0])\n","\n","    return image_set, X[shuffle], y[shuffle]\n"],"metadata":{"id":"dfB6FGEgNMDm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_dog_tensor(ksize, sigmas, r_sigmas):  # -> kx1tensor:\n","    # run a 5x5 gaussian blur then a 3x3 gaussian blr\n","    # blur5 = cv2.GaussianBlur(img,(5,5),0)\n","    # blur3 = cv2.GaussianBlur(img,(3,3),0)\n","\n","    # s1 = sp.filter.gaussian_filter(img, k*sigma)\n","    # s2 = sp.filter.gaussian_filter(img, sigma)\n","    # dog = s1 - s2\n","    n_kernels = len(sigmas) * len(r_sigmas) * 2\n","    kernels = []\n","    for sigma in sigmas:\n","        g_c = cv2.getGaussianKernel(ksize, sigma, ktype=cv2.CV_32F)\n","        for r_sigma in r_sigmas:\n","            g_s = cv2.getGaussianKernel(ksize, r_sigma*sigma, ktype=cv2.CV_32F)\n","            dog = g_c - g_s\n","            # dog /= np.sum(dog)  # Normalise\n","            dog = K.expand_dims(dog, -1)\n","            dog_on, dog_off = dog, dog * -1\n","            kernels.extend([dog_on, dog_off])\n","    assert len(kernels) == n_kernels\n","    print(f\"Created {n_kernels} kernels.\")\n","    return K.stack(kernels, axis=-1)\n","\n"],"metadata":{"id":"wHiwL_T4NWbh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_gabor_tensor(ksize, sigmas, thetas, lambdas, gammas, psis):\n","\n","    n_kernels = len(sigmas) * len(thetas) * len(lambdas) * len(gammas) * len(psis)\n","    gabors = []\n","    for sigma in sigmas:\n","        for theta in thetas:\n","            for lambd in lambdas:\n","                for gamma in gammas:\n","                    for psi in psis:\n","                        params = {'ksize': ksize, 'sigma': sigma,\n","                                  'theta': theta, 'lambd': lambd,\n","                                  'gamma': gamma, 'psi': psi}\n","                        gf = cv2.getGaborKernel(**params, ktype=cv2.CV_32F)\n","                        gf = K.expand_dims(gf, -1)\n","                        gabors.append(gf)\n","    assert len(gabors) == n_kernels\n","    print(f\"Created {n_kernels} kernels.\")\n","    return K.stack(gabors, axis=-1)\n","\n"],"metadata":{"id":"zquaHfsFNdVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convolve_tensor(x, kernel_tensor=None):\n","    '''\n","    conv2d\n","    input tensor: [batch, in_height, in_width, in_channels]\n","    kernel tensor: [filter_height, filter_width, in_channels, out_channels]\n","    '''\n","    # x = tf.image.rgb_to_grayscale(x)\n","    print(x.shape)\n","    print(kernel_tensor.shape)\n","    return K.conv2d(x, kernel_tensor, padding='same')\n"],"metadata":{"id":"gw5_kj2INlKh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def lambda_output_shape(input_shape):\n","     return input_shape"],"metadata":{"id":"uUT1mfPSFLRL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instantiate the parser\n","parser = argparse.ArgumentParser()\n","\n","parser.add_argument('--data_set', type=str, default='cifar10',\n","                    help='Data set to use')\n","parser.add_argument('--stimulus_set', type=str, default='static',\n","                    help='Stimulus set to use')\n","parser.add_argument('--noise_type', type=str, default=None,\n","                    help='Noise mask to use')\n","parser.add_argument('--trial_label', default='Trial1',\n","                    help='For labeling different runs of the same model')\n","parser.add_argument('--filter_type', type=str, default='gabor',\n","                    help='Convolutional filter type')\n","parser.add_argument('--filter_size', type=int, default=31,\n","                    help='Convolutional filter size')\n","parser.add_argument('--epochs', type=int, default=20,\n","                    help='Number of epochs to train model')\n","parser.add_argument('--data_augmentation', type=int, default=1,\n","                    help='Flag to use data augmentation in training')\n","parser.add_argument('--fresh_data', type=int, default=0,\n","                    help='Flag to (re)read images from files')\n","parser.add_argument('--model_name', type=str, default=None,\n","                    help='File name root to save outputs with')\n","parser.add_argument('--pretrained_model', type=str, default=None,\n","                    help='Pretrained model')\n","parser.add_argument('--n_gpus', type=int, default=1,\n","                    help='Number of GPUs to train across')\n","parser.add_argument('--save_loss', type=int, default=0,\n","                    help='Flag to save loss')\n","parser.add_argument('--lambd', type=float, default=None,\n","                    help='gabor sinusoid wavelength')\n","parser.add_argument('--sigma', type=float, default=None,\n","                    help='gabor/DoG Gaussian envelope standard deviation')\n","parser.add_argument('--r_sigma', type=float, default=None,\n","                    help='DoG ratio of standard deviations: sigma_s/sigma_c')\n","parser.add_argument(\"-f\", required=False)\n","\n","args = parser.parse_args()\n","\n","data_set = args.data_set\n","stimulus_set = args.stimulus_set\n","noise_type = args.noise_type\n","trial_label = args.trial_label\n","filter_type = args.filter_type.lower()\n","filter_size = args.filter_size\n","epochs = args.epochs\n","data_augmentation = args.data_augmentation\n","fresh_data = args.fresh_data\n","model_name = args.model_name\n","pretrained_model = args.pretrained_model\n","n_gpus = args.n_gpus\n","save_loss = args.save_loss\n","lambd = args.lambd\n","sigma = args.sigma\n","r_sigma = args.r_sigma\n","\n","weights = None  # 'imagenet'\n","pretrained_model = False\n","data_augmentation = False\n","batch_size = 64"],"metadata":{"id":"6qId93nFKvNM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2"],"metadata":{"id":"VWKWrgUaUB1k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_root = '/work/data/pixel/small'  # TODO: Pass in\n","input_shape = (128, 192, 1)  # (224, 224, 3)\n","# fresh_data = True\n","num_classes = 10\n","\n","project_root = os.path.realpath(os.pardir)\n","save_dir = os.path.join(project_root, 'results', filter_type, data_set, stimulus_set)\n","\n","if noise_type is None:\n","    noise_types = ['Original', 'Salt-and-pepper', 'Additive', 'Single-pixel']\n","else:  # Run with a single noise mask\n","    noise_types = [noise_type]\n","test_conditions = ['Same', 'Diff', 'NoPix']\n","\n","if filter_type.lower() == 'dog':\n","    # Difference of Gaussian parameters\n","    ksize = filter_size  # 31\n","    if sigma:\n","        sigmas = [sigma]\n","    else:\n","        sigmas = [1, 2, 3, 4]\n","    if r_sigma:\n","        r_sigmas = [r_sigma]\n","    else:\n","        r_sigmas = [1.1, 1.2, 1.5, 2, 2.5, 3]\n","\n","    # Generate DoG filters\n","    tensor = get_dog_tensor(ksize, sigmas, r_sigmas)\n","\n","elif filter_type.lower() == 'gabor':\n","    # Gabor filter parameters\n","    ksize = (filter_size, filter_size)  # (31, 31)\n","    if sigma:\n","        sigmas = [sigma]\n","        n_orients = 8\n","    else:\n","        sigmas = [2, 4]\n","        n_orients = 4\n","    # [0, np.pi/4, np.pi/2, np.pi*3/4]\n","    thetas = np.linspace(0, np.pi, n_orients, endpoint=False)\n","    if lambd:\n","        lambdas = [lambd]\n","    else:\n","        lambdas = [8, 16, 32, 64]\n","    n_phases = 4  # 1, 2, 4\n","    # [0, np.pi/2, np.pi, 3*np.pi/2]\n","    psis = np.linspace(0, 2*np.pi, n_phases, endpoint=False)\n","    n_ratios = 2  # 1, 2, 4\n","    gammas = np.linspace(1, 0, n_ratios, endpoint=False)\n","\n","    # Generate Gabor filters\n","    tensor = get_gabor_tensor(ksize, sigmas, thetas, lambdas, gammas, psis)\n","\n","else:\n","    sys.exit(f\"Unknown filter type requested: {filter_type}\")\n","\n","for noise_type in noise_types:\n","\n","    # model_name = f\"{data_set}_{stimulus_set}_{noise_type}_{trial_label}\"\n","    model_name = f\"{noise_type}_{trial_label}\"\n","    print(\"Running model:\", model_name)\n","\n","    if not os.path.isdir(save_dir):\n","        os.makedirs(save_dir)\n","    # model_path = os.path.join(save_dir, model_name)\n","\n","    # if data_set == 'cifar10':\n","    #     (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","    #     x_train = np.mean(x_train, 3, keepdims=True)  # Average over RGB channels\n","    #     x_test = np.mean(x_test, 3, keepdims=True)  # Average over RGB channels\n","    #     x_train = x_train.astype('float32')\n","    #     x_test = x_test.astype('float32')\n","    #     x_train /= 255\n","    #     x_test /= 255\n","\n","    #     # Convert class vectors to binary class matrices.\n","    #     y_train = keras.utils.to_categorical(y_train, num_classes)\n","    #     y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","    # elif data_set == 'pixel':\n","\n","    #   (noise_type, trial) = trial_label.split(\"_\")\n","\n","    if noise_type == 'Original':\n","        data_path = os.path.join(data_root, stimulus_set, 'orig')\n","    elif noise_type == 'Salt-and-pepper':\n","        data_path = os.path.join(data_root, stimulus_set, 'salt_n_pepper')\n","    elif noise_type == 'Additive':\n","        data_path = os.path.join(data_root, stimulus_set, 'uniform')\n","    elif noise_type == 'Single-pixel':\n","        data_path = os.path.join(data_root, stimulus_set, 'single_pixel')\n","    else:\n","        sys.exit(f\"Unknown noise type requested: {noise_type}\")\n","\n","    train_path = os.path.join(data_path, 'train')\n","    # test_path = os.path.join(data_path, f\"test_{noise_cond.lower()}\")\n","\n","    if os.path.isfile(os.path.join(train_path, 'x_train.npy')) and not fresh_data:\n","        print(f'Loading {data_set} data arrays.')\n","        x_train = np.load(os.path.join(train_path, 'x_train.npy'))\n","        y_train = np.load(os.path.join(train_path, 'y_train.npy'))\n","        # num_classes = len(os.listdir(train_path)) - 1\n","        cat_dirs = [os.path.join(train_path, o) for o in os.listdir(train_path)\n","                    if os.path.isdir(os.path.join(train_path, o))]\n","        assert num_classes == len(cat_dirs)\n","    else:\n","        print(f'Loading {data_set} image files.')\n","        train_images, x_train, y_train = load_images( )\n","        print(train_images.keys())\n","        assert num_classes == len(train_images)\n","        np.save(os.path.join(train_path, 'x_train.npy'), x_train)\n","        np.save(os.path.join(train_path, 'y_train.npy'), y_train)\n","\n","    # x_train = np.mean(x_train, 3, keepdims=True)  # Average over RGB channels\n","\n","    test_sets = []\n","    for test_cond in test_conditions:\n","        test_path = os.path.join(data_path, f\"test_{test_cond.lower()}\")\n","        if os.path.isfile(os.path.join(test_path, 'x_test.npy')) and not fresh_data:\n","            x_test = np.load(os.path.join(test_path, 'x_test.npy'))\n","            y_test = np.load(os.path.join(test_path, 'y_test.npy'))\n","        else:\n","            test_images, x_test, y_test = load_images(test_path)\n","            print(test_images.keys())\n","            assert num_classes == len(test_images)\n","            np.save(os.path.join(test_path, 'x_test.npy'), x_test)\n","            np.save(os.path.join(test_path, 'y_test.npy'), y_test)\n","        # test_sets.append((np.mean(x_test, 3, keepdims=True), y_test))\n","        test_sets.append((x_test, y_test))\n","    test_cond = \"NoPix\"  # Use this for examining learning curves\n","    x_test, y_test = test_sets[test_conditions.index(\"NoPix\")]  # Unpack default test set\n","    # else:\n","    #     sys.exit(f\"Unknown data set requested: {data_set}\")\n","\n","    # Summarise stimuli\n","    print('x_train shape:', x_train.shape)\n","    print(x_train.shape[0], 'train samples')\n","    print(x_test.shape[0], 'test samples')\n","    print(y_train.shape[1], 'training categories')\n","    print(y_test.shape[1], 'testing categories')\n","\n","    # Import VGG16\n","    model = VGG16(include_top=True, weights=weights, input_tensor=None,\n","                  input_shape=input_shape, pooling=None, classes=num_classes)\n","\n","    # Modify standard VGG16 with hardcoded Gabor convolutional layer\n","    layers = [l for l in model.layers]\n","    # x = layers[0].output\n","    inp = Input(shape=x_train[0].shape)\n","    print(inp.shape)\n","    x = Lambda(convolve_tensor, arguments={'kernel_tensor': tensor})\n","    name=(f\"{filter_type}_conv\")(inp)\n","    for layer in range(2, len(layers)):\n","        print(f\"L{layer} Input: {x.shape};\", end=' ')\n","        # print(layers[layer].get_config())\n","        if layer == 2 and x.shape[-1] != 64:\n","            # Replace block1_conv2 due to different number of channels\n","            # x = Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu', kernel_initializer={'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}})(x)\n","            x = Conv2D(**layers[layer].get_config())(x)\n","        else:\n","            x = layers[layer](x)\n","        print(f\"Output: {x.shape}\")\n","\n","    model = Model(inputs=inp, outputs=x)\n","    # model.summary()\n","\n","    if pretrained_model:\n","        # Load weights from saved model\n","        pretrained_model_path = os.path.join(save_dir, pretrained_model)\n","        model.load_weights(pretrained_model_path, by_name=True)\n","\n","        # Freeze weights in convolutional layers during training\n","        for layer in model.layers:\n","            if isinstance(layer, keras.layers.convolutional.Conv2D):\n","                print(f\"Freezing layer: {layer.name}\")\n","                layer.trainable = False\n","\n","    if n_gpus > 1:\n","        model = multi_gpu_model(model, gpus=n_gpus)\n","\n","    # initiate RMSprop optimizer\n","    opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n","\n","    # Compile the model last before training for all changes to take effect\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer=opt,\n","                  metrics=['accuracy'])\n","\n","    model.summary()\n","\n","    if not data_augmentation:\n","        print('Not using data augmentation.')\n","        hist = model.fit(x_train, y_train,\n","                         batch_size=batch_size,\n","                         epochs=epochs,\n","                         validation_data=(x_test, y_test),\n","                         shuffle=True)\n","\n","    else:\n","        print('Using data augmentation.')\n","        datagen = ImageDataGenerator(\n","                  featurewise_center=False,\n","                  samplewise_center=False,\n","                  featurewise_std_normalization=False,\n","                  samplewise_std_normalization=False,\n","                  zca_whitening=False,\n","                  rotation_range=0,\n","                  width_shift_range=0.1,\n","                  height_shift_range=0.1,\n","                  horizontal_flip=True,\n","                  vertical_flip=False)\n","\n","        datagen.fit(x_train)\n","\n","        hist = model.fit_generator(datagen.flow(x_train, y_train,\n","                                                batch_size=batch_size),\n","                                   steps_per_epoch=int(np.ceil(x_train.shape[0] / float(batch_size))),\n","                                   epochs=epochs,\n","                                   validation_data=(x_test, y_test),\n","                                   workers=4)\n","\n","    print('History', hist.history)\n","\n","    # Save model and weights\n","    if not os.path.isdir(save_dir):\n","        os.makedirs(save_dir)\n","    # model_name = 'SAVED'+'_'+model_name\n","    model_path = os.path.join(save_dir, model_name)\n","    # model.save(model_path)\n","    np.save(os.path.join(save_dir, f'{model_name}_VALACC.npy'), hist.history['val_acc'])\n","    np.save(os.path.join(save_dir, f'{model_name}_ACC.npy'), hist.history['acc'])\n","    if save_loss:\n","        np.save(os.path.join(save_dir, f'{model_name}_VALLOSS.npy'), hist.history['val_loss'])\n","        np.save(os.path.join(save_dir, f'{model_name}_LOSS.npy'), hist.history['loss'])\n","\n","    cond_acc = {}\n","    cond_loss = {}\n","    for test_cond, (x_test, y_test) in zip(test_conditions, test_sets):\n","        loss, val_acc = model.evaluate(x=x_test, y=y_test, batch_size=batch_size)\n","        cond_acc[test_cond] = val_acc\n","        cond_loss[test_cond] = loss\n","    print(\"Saving metrics: \", model.metrics_names)\n","    with open(os.path.join(save_dir, f'{model_name}_CONDVALACC.json'), \"w\") as jf:\n","        json.dump(cond_acc, jf)\n","    if save_loss:\n","        with open(os.path.join(save_dir, f'{model_name}_CONDVALLOSS.json'), \"w\") as jf:\n","            json.dump(cond_loss, jf)\n","\n","    print(f'Saved trained model at {model_path}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":537},"id":"T8LVbfE-FXLI","executionInfo":{"status":"error","timestamp":1687525644747,"user_tz":-330,"elapsed":1163,"user":{"displayName":"6112_Yash_Raj","userId":"06390485470456786871"}},"outputId":"41aecc5b-ac18-43c7-be4f-54a5092d2ba7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<>:152: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n","<>:152: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n","<ipython-input-21-27ee7c8dc09a>:152: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n","  name=(f\"{filter_type}_conv\")(inp)\n"]},{"output_type":"stream","name":"stdout","text":["Created 256 kernels.\n","Running model: Original_Trial1\n","Loading cifar10 image files.\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-27ee7c8dc09a>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Loading {data_set} image files.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/Brain Data/train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-84dc52e2dfe6>\u001b[0m in \u001b[0;36mload_images\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mn_cat_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mn_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cat_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     image_dims = plt.imread(os.path.join(path, categories[0],\n\u001b[0m\u001b[1;32m     14\u001b[0m                             image_set[categories[0]][0])).shape\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   2193\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1561\u001b[0m             \u001b[0;34m\"``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m             )\n\u001b[0;32m-> 1563\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mimg_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1564\u001b[0m         return (_pil_png_to_float_array(image)\n\u001b[1;32m   1565\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImagePlugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImageFile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2975\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2976\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/Brain Data/train/no/1 no.jpeg'"]}]}]}