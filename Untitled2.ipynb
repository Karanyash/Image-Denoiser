{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMgwVxcHo0rdWyzsUN4EhrU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":409},"id":"RKxMp9cyLVhZ","executionInfo":{"status":"error","timestamp":1687765144600,"user_tz":-330,"elapsed":3967,"user":{"displayName":"6112_Yash_Raj","userId":"06390485470456786871"}},"outputId":"39c2b15c-d6a0-487d-dd3e-74028d4732ea"},"outputs":[{"output_type":"stream","name":"stderr","text":["<>:334: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n","<>:334: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n"]},{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-fe6642ccca79>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                           \u001b[0mZeroPadding2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2DTranspose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                           GaussianNoise, UpSampling2D, Input)\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_gpu_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'Layer' from 'keras.engine' (/usr/local/lib/python3.10/dist-packages/keras/engine/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import os\n","import sys\n","import argparse\n","import json\n","\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from tqdm import tqdm\n","import keras\n","from keras import backend as K\n","from keras import activations, initializers, regularizers, constraints, metrics\n","from keras.datasets import cifar10\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential, Model\n","from keras.layers import (Dense, Dropout, Activation, Flatten, Reshape, Layer,\n","                          BatchNormalization, LocallyConnected2D,\n","                          ZeroPadding2D, Conv2D, MaxPooling2D, Conv2DTranspose,\n","                          GaussianNoise, UpSampling2D, Input)\n","from keras.engine import Layer, InputSpec\n","from keras.utils import conv_utils, multi_gpu_model\n","from keras.legacy import interfaces\n","\n","from keras.layers import Lambda\n","import tensorflow as tf\n","import cv2\n","from keras.applications.vgg16 import VGG16\n","\n","\n","def load_images(path):\n","\n","    image_set = {}\n","    for root, dirs, files in os.walk(path):\n","        if root == path:\n","            categories = sorted(dirs)\n","            image_set = {cat: [] for cat in categories}\n","        else:\n","            image_set[os.path.basename(root)] = sorted(files)\n","\n","    n_cat_images = {cat: len(files) for (cat, files) in image_set.items()}\n","    n_images = sum(n_cat_images.values())\n","    image_dims = plt.imread(os.path.join(path, categories[0],\n","                            image_set[categories[0]][0])).shape\n","\n","    print(image_dims)\n","    # X = np.zeros((n_images, *image_dims), dtype='float32')\n","    X = np.zeros((n_images, image_dims[0], image_dims[1], 1), dtype='float32')\n","    y = np.zeros((n_images, len(categories)), dtype=int)\n","    # y = np.zeros(n_images, dtype=int)\n","\n","    tally = 0\n","    for c, (cat, files) in enumerate(tqdm(image_set.items(), desc=path)):\n","        for i, image in enumerate(files):\n","            cimg = plt.imread(os.path.join(path, cat, image))\n","            X[i+tally] = np.expand_dims(cv2.cvtColor(cimg, cv2.COLOR_BGR2GRAY), axis=-1)\n","        y[tally:tally+len(files), c] = True\n","        tally += len(files)\n","\n","    shuffle = np.random.permutation(y.shape[0])\n","\n","    return image_set, X[shuffle], y[shuffle]\n","\n","\n","def get_dog_tensor(ksize, sigmas, r_sigmas):  # -> kx1tensor:\n","    # run a 5x5 gaussian blur then a 3x3 gaussian blr\n","    # blur5 = cv2.GaussianBlur(img,(5,5),0)\n","    # blur3 = cv2.GaussianBlur(img,(3,3),0)\n","\n","    # s1 = sp.filter.gaussian_filter(img, k*sigma)\n","    # s2 = sp.filter.gaussian_filter(img, sigma)\n","    # dog = s1 - s2\n","    n_kernels = len(sigmas) * len(r_sigmas) * 2\n","    kernels = []\n","    for sigma in sigmas:\n","        g_c = cv2.getGaussianKernel(ksize, sigma, ktype=cv2.CV_32F)\n","        for r_sigma in r_sigmas:\n","            g_s = cv2.getGaussianKernel(ksize, r_sigma*sigma, ktype=cv2.CV_32F)\n","            dog = g_c - g_s\n","            # dog /= np.sum(dog)  # Normalise\n","            dog = K.expand_dims(dog, -1)\n","            dog_on, dog_off = dog, dog * -1\n","            kernels.extend([dog_on, dog_off])\n","    assert len(kernels) == n_kernels\n","    print(f\"Created {n_kernels} kernels.\")\n","    return K.stack(kernels, axis=-1)\n","\n","\n","def get_gabor_tensor(ksize, sigmas, thetas, lambdas, gammas, psis):\n","\n","    n_kernels = len(sigmas) * len(thetas) * len(lambdas) * len(gammas) * len(psis)\n","    gabors = []\n","    for sigma in sigmas:\n","        for theta in thetas:\n","            for lambd in lambdas:\n","                for gamma in gammas:\n","                    for psi in psis:\n","                        params = {'ksize': ksize, 'sigma': sigma,\n","                                  'theta': theta, 'lambd': lambd,\n","                                  'gamma': gamma, 'psi': psi}\n","                        gf = cv2.getGaborKernel(**params, ktype=cv2.CV_32F)\n","                        gf = K.expand_dims(gf, -1)\n","                        gabors.append(gf)\n","    assert len(gabors) == n_kernels\n","    print(f\"Created {n_kernels} kernels.\")\n","    return K.stack(gabors, axis=-1)\n","\n","\n","def convolve_tensor(x, kernel_tensor=None):\n","    '''\n","    conv2d\n","    input tensor: [batch, in_height, in_width, in_channels]\n","    kernel tensor: [filter_height, filter_width, in_channels, out_channels]\n","    '''\n","    # x = tf.image.rgb_to_grayscale(x)\n","    print(x.shape)\n","    print(kernel_tensor.shape)\n","    return K.conv2d(x, kernel_tensor, padding='same')\n","\n","\n","# def lambda_output_shape(input_shape):\n","#     return input_shape\n","\n","\n","# Instantiate the parser\n","parser = argparse.ArgumentParser()\n","\n","parser.add_argument('--data_set', type=str, default='cifar10',\n","                    help='Data set to use')\n","parser.add_argument('--stimulus_set', type=str, default='static',\n","                    help='Stimulus set to use')\n","parser.add_argument('--noise_type', type=str, default=None,\n","                    help='Noise mask to use')\n","parser.add_argument('--trial_label', default='Trial1',\n","                    help='For labeling different runs of the same model')\n","parser.add_argument('--filter_type', type=str, default='gabor',\n","                    help='Convolutional filter type')\n","parser.add_argument('--filter_size', type=int, default=31,\n","                    help='Convolutional filter size')\n","parser.add_argument('--epochs', type=int, default=20,\n","                    help='Number of epochs to train model')\n","parser.add_argument('--data_augmentation', type=int, default=1,\n","                    help='Flag to use data augmentation in training')\n","parser.add_argument('--fresh_data', type=int, default=0,\n","                    help='Flag to (re)read images from files')\n","parser.add_argument('--model_name', type=str, default=None,\n","                    help='File name root to save outputs with')\n","parser.add_argument('--pretrained_model', type=str, default=None,\n","                    help='Pretrained model')\n","parser.add_argument('--n_gpus', type=int, default=1,\n","                    help='Number of GPUs to train across')\n","parser.add_argument('--save_loss', type=int, default=0,\n","                    help='Flag to save loss')\n","parser.add_argument('--lambd', type=float, default=None,\n","                    help='Gabor sinusoid wavelength')\n","parser.add_argument('--sigma', type=float, default=None,\n","                    help='Gabor/DoG Gaussian envelope standard deviation')\n","parser.add_argument('--r_sigma', type=float, default=None,\n","                    help='DoG ratio of standard deviations: sigma_s/sigma_c')\n","\n","args = parser.parse_args()\n","\n","data_set = args.data_set\n","stimulus_set = args.stimulus_set\n","noise_type = args.noise_type\n","trial_label = args.trial_label\n","filter_type = args.filter_type.lower()\n","filter_size = args.filter_size\n","epochs = args.epochs\n","data_augmentation = args.data_augmentation\n","fresh_data = args.fresh_data\n","model_name = args.model_name\n","pretrained_model = args.pretrained_model\n","n_gpus = args.n_gpus\n","save_loss = args.save_loss\n","lambd = args.lambd\n","sigma = args.sigma\n","r_sigma = args.r_sigma\n","\n","weights = None  # 'imagenet'\n","pretrained_model = False\n","data_augmentation = False\n","batch_size = 64\n","\n","data_root = '/work/data/pixel/small'  # TODO: Pass in\n","input_shape = (32, 32, 1)  # (224, 224, 3)\n","# fresh_data = True\n","num_classes = 10\n","\n","project_root = os.path.realpath(os.pardir)\n","save_dir = os.path.join(project_root, 'results', filter_type, data_set, stimulus_set)\n","\n","if noise_type is None:\n","    noise_types = ['Original', 'Salt-and-pepper', 'Additive', 'Single-pixel']\n","else:  # Run with a single noise mask\n","    noise_types = [noise_type]\n","test_conditions = ['Same', 'Diff', 'NoPix']\n","\n","if filter_type.lower() == 'dog':\n","    # Difference of Gaussian parameters\n","    ksize = filter_size  # 31\n","    if sigma:\n","        sigmas = [sigma]\n","    else:\n","        sigmas = [1, 2, 3, 4]\n","    if r_sigma:\n","        r_sigmas = [r_sigma]\n","    else:\n","        r_sigmas = [1.1, 1.2, 1.5, 2, 2.5, 3]\n","\n","    # Generate DoG filters\n","    tensor = get_dog_tensor(ksize, sigmas, r_sigmas)\n","\n","elif filter_type.lower() == 'gabor':\n","    # Gabor filter parameters\n","    ksize = (filter_size, filter_size)  # (31, 31)\n","    if sigma:\n","        sigmas = [sigma]\n","        n_orients = 8\n","    else:\n","        sigmas = [2, 4]\n","        n_orients = 4\n","    # [0, np.pi/4, np.pi/2, np.pi*3/4]\n","    thetas = np.linspace(0, np.pi, n_orients, endpoint=False)\n","    if lambd:\n","        lambdas = [lambd]\n","    else:\n","        lambdas = [8, 16, 32, 64]\n","    n_phases = 4  # 1, 2, 4\n","    # [0, np.pi/2, np.pi, 3*np.pi/2]\n","    psis = np.linspace(0, 2*np.pi, n_phases, endpoint=False)\n","    n_ratios = 2  # 1, 2, 4\n","    gammas = np.linspace(1, 0, n_ratios, endpoint=False)\n","\n","    # Generate Gabor filters\n","    tensor = get_gabor_tensor(ksize, sigmas, thetas, lambdas, gammas, psis)\n","\n","else:\n","    sys.exit(f\"Unknown filter type requested: {filter_type}\")\n","\n","for noise_type in noise_types:\n","\n","    # model_name = f\"{data_set}_{stimulus_set}_{noise_type}_{trial_label}\"\n","    model_name = f\"{noise_type}_{trial_label}\"\n","    print(\"Running model:\", model_name)\n","\n","    if not os.path.isdir(save_dir):\n","        os.makedirs(save_dir)\n","    # model_path = os.path.join(save_dir, model_name)\n","\n","    # if data_set == 'cifar10':\n","    #     (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","    #     x_train = np.mean(x_train, 3, keepdims=True)  # Average over RGB channels\n","    #     x_test = np.mean(x_test, 3, keepdims=True)  # Average over RGB channels\n","    #     x_train = x_train.astype('float32')\n","    #     x_test = x_test.astype('float32')\n","    #     x_train /= 255\n","    #     x_test /= 255\n","\n","    #     # Convert class vectors to binary class matrices.\n","    #     y_train = keras.utils.to_categorical(y_train, num_classes)\n","    #     y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","    # elif data_set == 'pixel':\n","\n","    #   (noise_type, trial) = trial_label.split(\"_\")\n","\n","    if noise_type == 'Original':\n","        data_path = os.path.join(data_root, stimulus_set, 'orig')\n","    elif noise_type == 'Salt-and-pepper':\n","        data_path = os.path.join(data_root, stimulus_set, 'salt_n_pepper')\n","    elif noise_type == 'Additive':\n","        data_path = os.path.join(data_root, stimulus_set, 'uniform')\n","    elif noise_type == 'Single-pixel':\n","        data_path = os.path.join(data_root, stimulus_set, 'single_pixel')\n","    else:\n","        sys.exit(f\"Unknown noise type requested: {noise_type}\")\n","\n","    train_path = os.path.join(data_path, 'train')\n","    # test_path = os.path.join(data_path, f\"test_{noise_cond.lower()}\")\n","\n","    if os.path.isfile(os.path.join(train_path, 'x_train.npy')) and not fresh_data:\n","        print(f'Loading {data_set} data arrays.')\n","        x_train = np.load(os.path.join(train_path, 'x_train.npy'))\n","        y_train = np.load(os.path.join(train_path, 'y_train.npy'))\n","        # num_classes = len(os.listdir(train_path)) - 1\n","        cat_dirs = [os.path.join(train_path, o) for o in os.listdir(train_path)\n","                    if os.path.isdir(os.path.join(train_path, o))]\n","        assert num_classes == len(cat_dirs)\n","    else:\n","        print(f'Loading {data_set} image files.')\n","        train_images, x_train, y_train = load_images(train_path)\n","        print(train_images.keys())\n","        assert num_classes == len(train_images)\n","        np.save(os.path.join(train_path, 'x_train.npy'), x_train)\n","        np.save(os.path.join(train_path, 'y_train.npy'), y_train)\n","\n","    # x_train = np.mean(x_train, 3, keepdims=True)  # Average over RGB channels\n","\n","    test_sets = []\n","    for test_cond in test_conditions:\n","        test_path = os.path.join(data_path, f\"test_{test_cond.lower()}\")\n","        if os.path.isfile(os.path.join(test_path, 'x_test.npy')) and not fresh_data:\n","            x_test = np.load(os.path.join(test_path, 'x_test.npy'))\n","            y_test = np.load(os.path.join(test_path, 'y_test.npy'))\n","        else:\n","            test_images, x_test, y_test = load_images(test_path)\n","            print(test_images.keys())\n","            assert num_classes == len(test_images)\n","            np.save(os.path.join(test_path, 'x_test.npy'), x_test)\n","            np.save(os.path.join(test_path, 'y_test.npy'), y_test)\n","        # test_sets.append((np.mean(x_test, 3, keepdims=True), y_test))\n","        test_sets.append((x_test, y_test))\n","    test_cond = \"NoPix\"  # Use this for examining learning curves\n","    x_test, y_test = test_sets[test_conditions.index(\"NoPix\")]  # Unpack default test set\n","    # else:\n","    #     sys.exit(f\"Unknown data set requested: {data_set}\")\n","\n","    # Summarise stimuli\n","    print('x_train shape:', x_train.shape)\n","    print(x_train.shape[0], 'train samples')\n","    print(x_test.shape[0], 'test samples')\n","    print(y_train.shape[1], 'training categories')\n","    print(y_test.shape[1], 'testing categories')\n","\n","    # Import VGG16\n","    model = VGG16(include_top=True, weights=weights, input_tensor=None,\n","                  input_shape=input_shape, pooling=None, classes=num_classes)\n","\n","    # Modify standard VGG16 with hardcoded Gabor convolutional layer\n","    layers = [l for l in model.layers]\n","    # x = layers[0].output\n","    inp = Input(shape=x_train[0].shape)\n","    print(inp.shape)\n","    x = Lambda(convolve_tensor, arguments={'kernel_tensor': tensor})\n","    name=(f\"{filter_type}_conv\")(inp)\n","    for layer in range(2, len(layers)):\n","        print(f\"L{layer} Input: {x.shape};\", end=' ')\n","        # print(layers[layer].get_config())\n","        if layer == 2 and x.shape[-1] != 64:\n","            # Replace block1_conv2 due to different number of channels\n","            # x = Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu', kernel_initializer={'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}})(x)\n","            x = Conv2D(**layers[layer].get_config())(x)\n","        else:\n","            x = layers[layer](x)\n","        print(f\"Output: {x.shape}\")\n","\n","    model = Model(inputs=inp, outputs=x)\n","    # model.summary()\n","\n","    if pretrained_model:\n","        # Load weights from saved model\n","        pretrained_model_path = os.path.join(save_dir, pretrained_model)\n","        model.load_weights(pretrained_model_path, by_name=True)\n","\n","        # Freeze weights in convolutional layers during training\n","        for layer in model.layers:\n","            if isinstance(layer, keras.layers.convolutional.Conv2D):\n","                print(f\"Freezing layer: {layer.name}\")\n","                layer.trainable = False\n","\n","    if n_gpus > 1:\n","        model = multi_gpu_model(model, gpus=n_gpus)\n","\n","    # initiate RMSprop optimizer\n","    opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n","\n","    # Compile the model last before training for all changes to take effect\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer=opt,\n","                  metrics=['accuracy'])\n","\n","    model.summary()\n","\n","    if not data_augmentation:\n","        print('Not using data augmentation.')\n","        hist = model.fit(x_train, y_train,\n","                         batch_size=batch_size,\n","                         epochs=epochs,\n","                         validation_data=(x_test, y_test),\n","                         shuffle=True)\n","\n","    else:\n","        print('Using data augmentation.')\n","        datagen = ImageDataGenerator(\n","            featurewise_center=False,\n","            samplewise_center=False,\n","            featurewise_std_normalization=False,\n","            samplewise_std_normalization=False,\n","            zca_whitening=False,\n","            rotation_range=0,\n","            width_shift_range=0.1,\n","            height_shift_range=0.1,\n","            horizontal_flip=True,\n","            vertical_flip=False)\n","\n","        datagen.fit(x_train)\n","\n","        hist = model.fit_generator(datagen.flow(x_train, y_train,\n","                                                batch_size=batch_size),\n","                                   steps_per_epoch=int(np.ceil(x_train.shape[0] / float(batch_size))),\n","                                   epochs=epochs,\n","                                   validation_data=(x_test, y_test),\n","                                   workers=4)\n","\n","    print('History', hist.history)\n","\n","    # Save model and weights\n","    if not os.path.isdir(save_dir):\n","        os.makedirs(save_dir)\n","    # model_name = 'SAVED'+'_'+model_name\n","    model_path = os.path.join(save_dir, model_name)\n","    # model.save(model_path)\n","    np.save(os.path.join(save_dir, f'{model_name}_VALACC.npy'), hist.history['val_acc'])\n","    np.save(os.path.join(save_dir, f'{model_name}_ACC.npy'), hist.history['acc'])\n","    if save_loss:\n","        np.save(os.path.join(save_dir, f'{model_name}_VALLOSS.npy'), hist.history['val_loss'])\n","        np.save(os.path.join(save_dir, f'{model_name}_LOSS.npy'), hist.history['loss'])\n","\n","    cond_acc = {}\n","    cond_loss = {}\n","    for test_cond, (x_test, y_test) in zip(test_conditions, test_sets):\n","        loss, val_acc = model.evaluate(x=x_test, y=y_test, batch_size=batch_size)\n","        cond_acc[test_cond] = val_acc\n","        cond_loss[test_cond] = loss\n","    print(\"Saving metrics: \", model.metrics_names)\n","    with open(os.path.join(save_dir, f'{model_name}_CONDVALACC.json'), \"w\") as jf:\n","        json.dump(cond_acc, jf)\n","    if save_loss:\n","        with open(os.path.join(save_dir, f'{model_name}_CONDVALLOSS.json'), \"w\") as jf:\n","            json.dump(cond_loss, jf)\n","\n","    print(f'Saved trained model at {model_path}')"]}]}